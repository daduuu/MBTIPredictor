{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice, AutoTokenizer, AutoModel\n",
    "from global_vars import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model_saved = \"bert_mlm22023-03-18-04-24-50.pt\"\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "class BertForClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5, freeze=False):\n",
    "\n",
    "        super(BertForClassification, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.freeze = freeze\n",
    "\n",
    "        #Freeze Last freeze_threshold Layers\n",
    "        if self.freeze:\n",
    "            for param in self.model.embeddings.parameters():\n",
    "                if(param.requires_grad):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "            for i in range(len(self.model.encoder.layer)):\n",
    "                if(i < len(self.model.encoder.layer) + freeze_threshold):\n",
    "                    for param in self.model.encoder.layer[i].parameters():\n",
    "                        if(param.requires_grad):\n",
    "                            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, o1 = self.model(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
    "        o2 = self.dropout(o1)\n",
    "        o3 = self.linear(o2)\n",
    "        fo = self.relu(o3)\n",
    "\n",
    "        return fo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertForClassification(freeze=True)\n",
    "model.load_state_dict(torch.load(model_saved, map_location=torch.device('cpu')))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "labels = {0: 0,\n",
    "          1: 1,\n",
    "          2: 2,\n",
    "            3: 3,\n",
    "            4: 4,\n",
    "            5: 5,\n",
    "            6: 6,\n",
    "            7: 7,\n",
    "            8: 8,\n",
    "            9: 9,\n",
    "            10: 10,\n",
    "            11: 11,\n",
    "            12: 12,\n",
    "            13: 13,\n",
    "            14: 14,\n",
    "            15: 15\n",
    "          }\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['type']]\n",
    "        self.posts = [tokenizer(post, \n",
    "                               padding=doPadding, max_length = max_length_input, truncation=doTruncate,\n",
    "                                return_tensors=\"pt\") for post in df['posts']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.posts[idx], np.array(self.labels[idx])\n",
    "\n",
    "\n",
    "ds = load_from_disk(\"BERT_TEST\")\n",
    "df = pd.DataFrame.from_dict({\"posts\": ds['posts'], \"type\": ds['type']})\n",
    "\n",
    "df_test = Dataset(df)\n",
    "test_dataloader = torch.utils.data.DataLoader(df_test, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 10.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "total_acc_test = 0\n",
    "\n",
    "\n",
    "for test_input, test_label in tqdm(test_dataloader):\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.26\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Acc: \" + str(total_acc_test / len(df_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
