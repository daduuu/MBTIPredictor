{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_vars import *\n",
    "from transformers import AutoModelForMultipleChoice, AutoTokenizer\n",
    "from datasets import Dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import pickle\n",
    "import numpy as np\n",
    "from wandb.keras import WandbMetricsLogger, WandbCallback\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import Model # if only machine learning were this easy :P\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import any other libraries you want here:\n",
    "from tensorflow.keras.layers import Dense, SpatialDropout1D, LSTM, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 1\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\enoch\\MBTIPredictor\\wandb\\run-20230318_162402-dqpglnce</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mbtipredictor/mbti_bert_mlm/runs/dqpglnce' target=\"_blank\">colorful-sponge-366</a></strong> to <a href='https://wandb.ai/mbtipredictor/mbti_bert_mlm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mbtipredictor/mbti_bert_mlm' target=\"_blank\">https://wandb.ai/mbtipredictor/mbti_bert_mlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mbtipredictor/mbti_bert_mlm/runs/dqpglnce' target=\"_blank\">https://wandb.ai/mbtipredictor/mbti_bert_mlm/runs/dqpglnce</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"mbti_bert_mlm\",\n",
    "    config=dict,\n",
    "    entity=\"mbtipredictor\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 106067 entries, 0 to 106066\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   posts    106067 non-null  object\n",
      " 1   type     106067 non-null  object\n",
      " 2   new_col  106067 non-null  int64 \n",
      " 3   pad      106067 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"converted_new_for_custom.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256681 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['posts'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (106067, 500)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['posts'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (106067, 16)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['type']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95460, 500) (95460, 16)\n",
      "(10607, 500) (10607, 16)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85914, 500) (85914, 16)\n",
      "(9546, 500) (9546, 16)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_val.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "xt= X_train[:500]\n",
    "xv = X_val[:50]\n",
    "xte = X_test[:50]\n",
    "yt= Y_train[:500]\n",
    "yv = Y_val[:50]\n",
    "yte = Y_test[:50]\n",
    "\n",
    "print(len(xt))\n",
    "print(len(xv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, 500, 100)          5000000   \n",
      "                                                                 \n",
      " spatial_dropout1d_25 (Spati  (None, 500, 100)         0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 500, 100)          80400     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 500, 50)           30200     \n",
      "                                                                 \n",
      " globalaveragepooling1d (Glo  (None, 50)               0         \n",
      " balAveragePooling1D)                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                816       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,111,416\n",
      "Trainable params: 5,111,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, name = \"lstm_1\", return_sequences=True))\n",
    "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2, name = \"lstm_2\", return_sequences=True))\n",
    "model.add(GlobalAveragePooling1D(name = \"globalaveragepooling1d\"))\n",
    "model.add(Dense(16, activation='softmax', name = \"dense\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((xt, yt))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((xv, yv))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "\n",
      "Start of epoch 0\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000002BC73F10820>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\enoch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\backend.py\", line 5133, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"C:\\Users\\enoch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "Training loss (for one batch) at step 0: 2.7728\n",
      "Seen so far: 10 samples\n",
      "Validation loss (for 200 batch) at step 0: 2.7657\n",
      "Training loss (for one batch) at step 1: 2.7676\n",
      "Seen so far: 20 samples\n",
      "Training loss (for one batch) at step 2: 2.7606\n",
      "Seen so far: 30 samples\n",
      "Training loss (for one batch) at step 3: 2.7530\n",
      "Seen so far: 40 samples\n",
      "Training loss (for one batch) at step 4: 2.7495\n",
      "Seen so far: 50 samples\n",
      "Validation loss (for 200 batch) at step 4: 2.7187\n",
      "Training loss (for one batch) at step 5: 2.7204\n",
      "Seen so far: 60 samples\n",
      "Training loss (for one batch) at step 6: 2.7346\n",
      "Seen so far: 70 samples\n",
      "Training loss (for one batch) at step 7: 2.7414\n",
      "Seen so far: 80 samples\n",
      "Training loss (for one batch) at step 8: 2.6481\n",
      "Seen so far: 90 samples\n",
      "Validation loss (for 200 batch) at step 8: 2.5967\n",
      "Training loss (for one batch) at step 9: 2.7029\n",
      "Seen so far: 100 samples\n",
      "Training loss (for one batch) at step 10: 2.5726\n",
      "Seen so far: 110 samples\n",
      "Training loss (for one batch) at step 11: 2.6025\n",
      "Seen so far: 120 samples\n",
      "Training loss (for one batch) at step 12: 2.5162\n",
      "Seen so far: 130 samples\n",
      "Validation loss (for 200 batch) at step 12: 2.0627\n",
      "Training loss (for one batch) at step 13: 2.5563\n",
      "Seen so far: 140 samples\n",
      "Training loss (for one batch) at step 14: 2.3016\n",
      "Seen so far: 150 samples\n",
      "Training loss (for one batch) at step 15: 2.5272\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 16: 2.6983\n",
      "Seen so far: 170 samples\n",
      "Validation loss (for 200 batch) at step 16: 1.9409\n",
      "Training loss (for one batch) at step 17: 2.4247\n",
      "Seen so far: 180 samples\n",
      "Training loss (for one batch) at step 18: 1.8941\n",
      "Seen so far: 190 samples\n",
      "Training loss (for one batch) at step 19: 1.8973\n",
      "Seen so far: 200 samples\n",
      "Training loss (for one batch) at step 20: 2.1894\n",
      "Seen so far: 210 samples\n",
      "Validation loss (for 200 batch) at step 20: 2.0103\n",
      "Training loss (for one batch) at step 21: 2.5075\n",
      "Seen so far: 220 samples\n",
      "Training loss (for one batch) at step 22: 2.2714\n",
      "Seen so far: 230 samples\n",
      "Training loss (for one batch) at step 23: 1.9081\n",
      "Seen so far: 240 samples\n",
      "Training loss (for one batch) at step 24: 2.4164\n",
      "Seen so far: 250 samples\n",
      "Validation loss (for 200 batch) at step 24: 1.9630\n",
      "Training loss (for one batch) at step 25: 2.5804\n",
      "Seen so far: 260 samples\n",
      "Training loss (for one batch) at step 26: 2.5658\n",
      "Seen so far: 270 samples\n",
      "Training loss (for one batch) at step 27: 2.6839\n",
      "Seen so far: 280 samples\n",
      "Training loss (for one batch) at step 28: 1.9248\n",
      "Seen so far: 290 samples\n",
      "Validation loss (for 200 batch) at step 28: 1.9801\n",
      "Training loss (for one batch) at step 29: 2.1400\n",
      "Seen so far: 300 samples\n",
      "Training loss (for one batch) at step 30: 2.5022\n",
      "Seen so far: 310 samples\n",
      "Training loss (for one batch) at step 31: 1.8169\n",
      "Seen so far: 320 samples\n",
      "Training loss (for one batch) at step 32: 1.8663\n",
      "Seen so far: 330 samples\n",
      "Validation loss (for 200 batch) at step 32: 1.9643\n",
      "Training loss (for one batch) at step 33: 2.0989\n",
      "Seen so far: 340 samples\n",
      "Training loss (for one batch) at step 34: 2.5390\n",
      "Seen so far: 350 samples\n",
      "Training loss (for one batch) at step 35: 1.7573\n",
      "Seen so far: 360 samples\n",
      "Training loss (for one batch) at step 36: 1.9804\n",
      "Seen so far: 370 samples\n",
      "Validation loss (for 200 batch) at step 36: 1.9217\n",
      "Training loss (for one batch) at step 37: 2.1364\n",
      "Seen so far: 380 samples\n",
      "Training loss (for one batch) at step 38: 2.0259\n",
      "Seen so far: 390 samples\n",
      "Training loss (for one batch) at step 39: 2.3744\n",
      "Seen so far: 400 samples\n",
      "Training loss (for one batch) at step 40: 2.1380\n",
      "Seen so far: 410 samples\n",
      "Validation loss (for 200 batch) at step 40: 1.8861\n",
      "Training loss (for one batch) at step 41: 2.6250\n",
      "Seen so far: 420 samples\n",
      "Training loss (for one batch) at step 42: 2.4691\n",
      "Seen so far: 430 samples\n",
      "Training loss (for one batch) at step 43: 2.8287\n",
      "Seen so far: 440 samples\n",
      "Training loss (for one batch) at step 44: 2.1439\n",
      "Seen so far: 450 samples\n",
      "Validation loss (for 200 batch) at step 44: 1.8559\n",
      "Training loss (for one batch) at step 45: 2.2241\n",
      "Seen so far: 460 samples\n",
      "Training loss (for one batch) at step 46: 2.7338\n",
      "Seen so far: 470 samples\n",
      "Training loss (for one batch) at step 47: 2.3441\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 48: 1.9289\n",
      "Seen so far: 490 samples\n",
      "Validation loss (for 200 batch) at step 48: 1.8610\n",
      "Training loss (for one batch) at step 49: 1.9146\n",
      "Seen so far: 500 samples\n",
      "Training acc over epoch: 0.2040\n",
      "Validation acc: 0.2614\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    val_loss_value = loss_fn(y, val_logits)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "    return val_loss_value\n",
    "\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # Log every 1 batches.\n",
    "        if step % 1 == 0:\n",
    "            train_acc = train_acc_metric.result()\n",
    "            #train_acc_metric.reset_states()\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "            wandb.log({\"Training Loss\": loss_value, \"Train Acc\": train_acc})\n",
    "        # Run a validation loop each 4 steps\n",
    "        if step % 4 == 0:\n",
    "            for x_batch_val, y_batch_val in val_dataset:\n",
    "                val_loss = test_step(x_batch_val, y_batch_val)\n",
    "                val_logits = model(x_batch_val, training=False)\n",
    "            val_acc = val_acc_metric.result()\n",
    "            val_acc_metric.reset_states()\n",
    "            print(\n",
    "                \"Validation loss (for 200 batch) at step %d: %.4f\"\n",
    "                % (step, float(val_loss))\n",
    "            )\n",
    "            wandb.log({\"Validation Loss\": val_loss, \"Validation Acc\": val_acc})\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "    wandb.log({\"Train Acc Epoch\": train_acc})\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    wandb.log({\"Validation Acc Epoch\": val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 339 calls to <function Model.make_test_function.<locals>.test_function at 0x000002BC78743670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 144ms/step - loss: 2.0157 - categorical_accuracy: 0.2800\n",
      "Test set\n",
      "  Loss: 2.016\n",
      "  Accuracy: 0.280\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(xte,yte)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file saved_model already exists.\n",
      "Error occurred while processing: saved_model.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-sponge-366</strong> at: <a href='https://wandb.ai/mbtipredictor/mbti_bert_mlm/runs/dqpglnce' target=\"_blank\">https://wandb.ai/mbtipredictor/mbti_bert_mlm/runs/dqpglnce</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230318_162402-dqpglnce\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
